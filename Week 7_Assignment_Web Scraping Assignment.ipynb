{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ea4ce7-def9-4bc6-8f88-dd3f96ced527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "# Answer 1 - \n",
    "# Web scraping is an automatic method to obtain large amounts of data from websites. \n",
    "\n",
    "# Web scraping is used to collect data which is mostly unstructured data in an HTML format which is then converted into structured data # in a spreadsheet or a database\n",
    "# so that it can be used in various applications.\n",
    "\n",
    "# Three uses of Web Scraping: -\n",
    "\n",
    "# 1. Collecting Product review feedback scores and comments : - Web scraping could be used to collect product review feedback scores and comments from e-commerce websites.\n",
    "\n",
    "# 2. E-mail Marketing: - Web scraping could be used for email marketing. Web scraping could be used to collect Email ID’s from various sites using web scraping \n",
    "#    and then send bulk promotional and marketing Emails to all the people owning these Email ID’s.\n",
    "\n",
    "# 3. News Monitoring - Web scraping news sites can provide detailed reports on the current news to a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0959a5-5267-4f6c-93b0-458777e26a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "# Answer 2 - different methods used for Web Scraping?: -\n",
    "\n",
    "# 1. HTML Parsing - HTML parsing involves the use of code to target a linear or nested HTML page.\n",
    "#    It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "# 2. DOM Parsing - The Document Object Model (DOM) defines the structure, style and content of an XML file. \n",
    "#    Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information \n",
    "#    and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer \n",
    "#    to extract whole web pages (or parts of them).\n",
    "\n",
    "# 3. Vertical Aggregation - Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals.\n",
    "#    These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals \n",
    "#    with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the \n",
    "#    quality of data they extract.\n",
    "\n",
    "# 4. XPath - XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, \n",
    "#    so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. \n",
    "#    A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "# 5. Google Sheets - Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website,\n",
    "#    which is useful if they want to extract a specific pattern or data from the website. \n",
    "#    This command also makes it possible to check if a website can be scraped or is protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f43b8fd-146a-4452-b202-ec0205f05363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 - \n",
    "\n",
    "# Answer 3 - Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you interact with a web page \n",
    "# using developer tools. # The library exposes a couple of intuitive functions you can use to explore the HTML you received.\n",
    "\n",
    "# Use of Beautiful soup - It transforms a complex HTML document into a tree of Python objects.\n",
    "# It also automatically converts the document to Unicode, so you don't have to think about encodings.\n",
    "#  It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2386a4f-9805-4d02-b018-a57d5dfa1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 - Why is flask used in this Web Scraping project?\n",
    "\n",
    "# Answer 4 - flask is used in web scraping project to create and render a custom html page to receive and process the requests of the key word strings to be used for\n",
    "# the web scraping project. Flask is used to create a running app server for receiving the requests from the client html pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2705cb1-0878-4375-a291-a229f487657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 - \n",
    "\n",
    "# Answer 5 - names of AWS services used in the web scraping project: -\n",
    "# 1. AWS Code Pipeline - AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software.\n",
    "#    AWS Code Pipeline is used to get the codes stored from github portal to AWS which can then be used AWS Elastic Bean Stalk\n",
    "\n",
    "# 2. AWS Elastic Beanstalk - Elastic Beanstalk is a service for deploying and scaling web applications and services.\n",
    "#    Code uploaded in code pipeline from Github portal is used by Elastic Beanstalk.Elastic Beanstalk automatically handles the deployment, \n",
    "#    from capacity provisioning, load balancing, and auto scaling to application health monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2273b2d-9a55-439c-bcd0-705920da4e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
