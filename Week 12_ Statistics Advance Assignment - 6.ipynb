{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa24d6ce-4d36-4ffc-8672-15d4f6e36f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "# Answer 1 -\n",
    "\n",
    "# Analysis of Variance (ANOVA) is a statistical technique used to compare means of three or more groups to determine if there are significant \n",
    "# differences between them. To use ANOVA and obtain reliable results, certain assumptions need to be met. \n",
    "# These assumptions ensure that the test is appropriate and the conclusions drawn are valid. \n",
    "# The assumptions for ANOVA include:\n",
    "\n",
    "# 1. Normality: The data within each group should follow a normal distribution. This assumption is important because ANOVA relies on the \n",
    "# normal distribution to estimate population parameters accurately.\n",
    "\n",
    "# 2. Homogeneity of Variance: The variances of the groups being compared should be approximately equal. Unequal variances can affect the \n",
    "# overall F-test's sensitivity and result in inaccurate p-values.\n",
    "\n",
    "# 3. Independence: Observations within each group should be independent of each other. This assumption ensures that the observations are \n",
    "# not correlated or related to each other within a group.\n",
    "\n",
    "# 4. Absence of Outliers: Oulying score need to be removed from the dataset\n",
    "\n",
    "# Examples of Violations and Their Impact:\n",
    "\n",
    "# 1. Normality Violation:\n",
    "#   - Impact: If the assumption of normality is violated, the validity of p-values and confidence intervals can be compromised. \n",
    "#     The F-test's distribution relies on the normality assumption, so violating it may lead to incorrect conclusions about group differences.\n",
    "#   - Example: In an ANOVA comparing test scores between groups, if one group's scores are heavily skewed, it might violate the normality assumption.\n",
    "\n",
    "# 2. Homogeneity of Variance Violation:\n",
    "#   - Impact: Violating the homogeneity of variance assumption can lead to inflated or deflated p-values. If variances are not equal, \n",
    "#     the F-test's assumptions are not met, potentially leading to a higher Type I error rate.\n",
    "#   - Example: In a study comparing the effects of different fertilizer brands on plant growth, if the variances in plant heights are vastly \n",
    "#    different between the fertilizer groups, the homogeneity of variance assumption might be violated.\n",
    "\n",
    "# 3. Independence Violation:\n",
    "#   - Impact: If observations are not independent within groups, the assumption of independence is violated. This can lead to incorrect conclusions \n",
    "#     about group differences and affect the validity of the F-test.\n",
    "#   - Example: In a study where students' test scores are measured before and after a tutoring program, the scores within each student are not \n",
    "#    independent, violating the assumption.\n",
    "\n",
    "# When these assumptions are significantly violated, the results of ANOVA may be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f73859-d675-4ade-94b8-4d1221706297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "# Answer 2 -\n",
    "\n",
    "# There are three main types of Analysis of Variance (ANOVA) techniques: One-Way ANOVA, Two-Way ANOVA, and Multivariate ANOVA (MANOVA). \n",
    "# Each type is used in different situations to analyze and compare means across different groups or factors.\n",
    "\n",
    "# 1. One-Way ANOVA:\n",
    "#   - Situation: One-Way ANOVA is used when you have one independent variable with more than two levels (groups) and you want to determine \n",
    "#    if there are any significant differences in means between these groups.\n",
    "#   - Example: Suppose you want to compare the mean test scores of students from three different schools to determine if there's a significant \n",
    "#    difference in the quality of education they receive.\n",
    "\n",
    "# 2. Two-Way ANOVA:\n",
    "#    - Situation: Two-Way ANOVA is used when you have two independent variables (factors) and you want to analyze their main effects and \n",
    "#     potential interaction effects on a dependent variable.\n",
    "#   - Example: Consider a study where you're investigating the effects of both gender and diet on weight loss. You have two independent variables: \n",
    "#     gender (male/female) and diet type (low-carb/high-carb). Two-Way ANOVA helps you assess if there are main effects for each factor and \n",
    "#     whether the interaction of gender and diet is significant.\n",
    "\n",
    "# 3. Multivariate ANOVA (MANOVA):\n",
    "#   - Situation: MANOVA is used when you have two or more dependent variables and you want to determine if there are significant differences \n",
    "#    among groups on these multiple dependent variables simultaneously.\n",
    "#  - Example: Imagine a study examining the effects of different exercise programs on both weight loss and cardiovascular fitness. \n",
    "#    You're interested in understanding if the exercise programs have a joint effect on both variables. MANOVA helps you analyze whether there are \n",
    "#    any significant differences in the combination of weight loss and fitness levels.\n",
    "\n",
    "# These ANOVA techniques are used to analyze different levels of complexity in experimental designs. \n",
    "# One-Way ANOVA is suitable for comparing means across multiple groups of one factor. \n",
    "# Two-Way ANOVA expands this to two factors, and it allows you to examine potential interactions between these factors. \n",
    "# MANOVA goes further by considering multiple dependent variables, giving insight into patterns across multiple outcome measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0437c3-391a-417b-9757-b092a915321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "\n",
    "# Answer 3 -\n",
    "\n",
    "# The partitioning of variance in ANOVA refers to the process of decomposing the total variability observed in a dataset into different \n",
    "# components that can be attributed to different sources or factors. This partitioning helps to understand how much of the variability in the data \n",
    "# is due to the factors being studied and how much is due to random variability or measurement error.\n",
    "\n",
    "#In ANOVA, the total variability is broken down into two main components:\n",
    "\n",
    "#1. Between-Group Variability (Treatment Variability):\n",
    "#   This component represents the variability between the group means. It indicates how much the means of different groups differ from each other. \n",
    "#   The larger this component is relative to the total variability, the more likely it is that the group means are significantly different.\n",
    "\n",
    "# 2. Within-Group Variability (Error Variability):\n",
    "#   This component represents the variability within each group. It indicates how much individual data points within each group deviate from their \n",
    "#   respective group mean. This variability is often attributed to random chance or measurement error.\n",
    "\n",
    "# The partitioning of variance is represented mathematically using the sum of squares (SS) terms. There are three primary sources of sums of squares \n",
    "# in a one-way ANOVA:\n",
    "\n",
    "# 1. Total Sum of Squares (SSTotal):\n",
    "#   This represents the total variability in the dataset, calculated as the sum of squared differences between each data point and the overall mean \n",
    "# of all data points.\n",
    "\n",
    "# 2. Between-Group Sum of Squares (SSBetween):\n",
    "#   This represents the variability between the group means, calculated as the sum of squared differences between each group mean and the overall \n",
    "#   mean.\n",
    "\n",
    "# 3. Within-Group Sum of Squares (SSWithin):\n",
    "#   This represents the variability within each group, calculated as the sum of squared differences between each data point and its group mean.\n",
    "\n",
    "# The importance of understanding the partitioning of variance in ANOVA includes:\n",
    "\n",
    "#1. Interpretation of Results:\n",
    "#   By understanding how much of the variability is due to between-group differences and how much is due to within-group variability, \n",
    "#  researchers can better interpret the significance of the differences between groups. It provides insight into the factors that contribute to the \n",
    "#  observed variation.\n",
    "\n",
    "# 2. Validity of Results:\n",
    "#   A large between-group variability relative to within-group variability indicates that the groups might be significantly different. \n",
    "#  However, if the within-group variability is too large, it might mask genuine between-group differences.\n",
    "\n",
    "# 3. Model Assessment:\n",
    "#   Partitioning the variance allows researchers to assess how well the model (ANOVA) explains the observed variability. A good model should \n",
    "#  capture most of the variability between groups while minimizing the variability within groups.\n",
    "\n",
    "# 4. Comparing Effects:\n",
    "#   Understanding the proportion of variance explained by different factors helps researchers compare the relative importance of these factors \n",
    "#  in explaining the observed variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68f7c2f0-754a-482c-b0fe-1f00f499f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 499.73333333333335\n",
      "Explained Sum of Squares (SSE): 416.13333333333316\n",
      "Residual Sum of Squares (SSR): 83.6000000000002\n"
     ]
    }
   ],
   "source": [
    "# Question 4 -\n",
    "\n",
    "# Answer 4 -\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for each group\n",
    "group_1 = np.array([22, 18, 20, 25, 23])\n",
    "group_2 = np.array([30, 28, 25, 33, 31])\n",
    "group_3 = np.array([17, 15, 14, 19, 18])\n",
    "\n",
    "# Overall data\n",
    "all_data = np.concatenate([group_1, group_2, group_3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = [np.mean(group) for group in [group_1, group_2, group_3]]\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group_1, group_2, group_3], group_means)])\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453b3228-ea39-451b-a352-5131a1176b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: [-7.77777778  0.22222222  7.55555556]\n",
      "Main Effect B: [-2.44444444 -0.11111111  2.55555556]\n",
      "Interaction Effect: [[-20.         -20.33333333 -20.        ]\n",
      " [-20.         -20.33333333 -20.        ]\n",
      " [-20.33333333 -19.66666667 -20.33333333]]\n",
      "F-ratio for Main Effect A: 0.06462180171931432\n",
      "F-ratio for Main Effect B: 0.006877257235871611\n",
      "P-value for Main Effect A: 0.9383804976399921\n",
      "P-value for Main Effect B: 0.9931580533249532\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "# Answer 5 -\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[10, 12, 15],\n",
    "                 [18, 20, 23],\n",
    "                 [25, 28, 30]])\n",
    "\n",
    "# Calculate means for Factor A and Factor B\n",
    "mean_factor_a = np.mean(data, axis=1)\n",
    "mean_factor_b = np.mean(data, axis=0)\n",
    "\n",
    "# Calculate Grand Mean (overall mean)\n",
    "grand_mean = np.mean(data)\n",
    "\n",
    "# Calculate Main Effect for Factor A\n",
    "main_effect_a = mean_factor_a - grand_mean\n",
    "\n",
    "# Calculate Main Effect for Factor B\n",
    "main_effect_b = mean_factor_b - grand_mean\n",
    "\n",
    "# Calculate Interaction Effect\n",
    "interaction_effect = data - (mean_factor_a[:, np.newaxis] + mean_factor_b)\n",
    "\n",
    "# Calculate the sums of squares for Main Effect A, Main Effect B, and Interaction Effect\n",
    "sse_main_effect_a = np.sum(main_effect_a**2)\n",
    "sse_main_effect_b = np.sum(main_effect_b**2)\n",
    "sse_interaction_effect = np.sum(interaction_effect**2)\n",
    "\n",
    "# Calculate the degrees of freedom for each effect\n",
    "df_main_effect_a = data.shape[0] - 1\n",
    "df_main_effect_b = data.shape[1] - 1\n",
    "df_interaction_effect = (data.shape[0] - 1) * (data.shape[1] - 1)\n",
    "\n",
    "# Calculate mean squares for each effect\n",
    "ms_main_effect_a = sse_main_effect_a / df_main_effect_a\n",
    "ms_main_effect_b = sse_main_effect_b / df_main_effect_b\n",
    "ms_interaction_effect = sse_interaction_effect / df_interaction_effect\n",
    "\n",
    "# Calculate F-ratio for each effect\n",
    "f_ratio_main_effect_a = ms_main_effect_a / ms_interaction_effect\n",
    "f_ratio_main_effect_b = ms_main_effect_b / ms_interaction_effect\n",
    "\n",
    "# Calculate p-values for each effect\n",
    "p_value_main_effect_a = 1 - f.cdf(f_ratio_main_effect_a, df_main_effect_a, df_interaction_effect)\n",
    "p_value_main_effect_b = 1 - f.cdf(f_ratio_main_effect_b, df_main_effect_b, df_interaction_effect)\n",
    "\n",
    "print(\"Main Effect A:\", main_effect_a)\n",
    "print(\"Main Effect B:\", main_effect_b)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n",
    "print(\"F-ratio for Main Effect A:\", f_ratio_main_effect_a)\n",
    "print(\"F-ratio for Main Effect B:\", f_ratio_main_effect_b)\n",
    "print(\"P-value for Main Effect A:\", p_value_main_effect_a)\n",
    "print(\"P-value for Main Effect B:\", p_value_main_effect_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b99e5-ed18-4285-afcc-3dbb908e7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6\n",
    "\n",
    "# Answer 6 -\n",
    "\n",
    "# The F-statistic is a ratio of the variability between group means to the variability within groups. \n",
    "# A larger F-statistic indicates that the differences between group means are relatively large compared to the within-group variability.\n",
    "# F-statistic of 5.23 indicates that the differences between group means are relatively large compared to the within-group variability.\n",
    "\n",
    "# The p-value is a measure of the evidence against the null hypothesis. It indicates the probability of observing such an \n",
    "# extreme F-statistic (or more extreme) if the null hypothesis were true. A smaller p-value suggests stronger evidence against the null hypothesis.\n",
    "# Since the p-value of 0.02 is less than the common significance level of 0.05, we have evidence to reject the null hypothesis. \n",
    "\n",
    "# In the context of a one-way ANOVA:\n",
    "# Null Hypothesis (H0): The group means are all equal (there are no significant differences between groups).\n",
    "# Alternate Hypothesis (H1): At least one group mean is different from the others.\n",
    "\n",
    "# Final conclusion: Basis the above F-statistic of 5.23 and p-value of 0.02; we have evidence to not accept the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a815701-3a92-4bc0-bba2-0c9163ab46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7\n",
    "\n",
    "# Answer 7 -\n",
    "\n",
    "# In a repeated measures ANOVA, missing data can be a common challenge. Handling missing data appropriately is crucial to ensure the validity \n",
    "# and reliability of the analysis. There are several methods to handle missing data, each with its potential consequences:\n",
    "\n",
    "# 1. Listwise Deletion (Complete Case Analysis):\n",
    "#   - This method involves excluding any participant with missing data from the analysis.\n",
    "#   - Consequences: This can lead to reduced sample size, loss of statistical power, and potential bias if the missing data are not completely \n",
    "#    random (i.e., if they are related to the variables being studied).\n",
    "\n",
    "# 2. Pairwise Deletion (Available Case Analysis):\n",
    "#   - This method includes all available data points for each specific analysis.\n",
    "#   - Consequences: Similar to listwise deletion, it can result in reduced sample size and loss of statistical power. Different analyses \n",
    "#    might have different effective sample sizes, leading to potential inconsistencies in results.\n",
    "\n",
    "# 3. Mean Substitution (Imputation):\n",
    "#   - Replace missing values with the mean value of the variable.\n",
    "#   - Consequences: This method can artificially reduce variability and potentially bias results if missingness is not random. It can also \n",
    "#    underestimate standard errors, leading to incorrect significance tests.\n",
    "\n",
    "# 4. Last Observation Carried Forward (LOCF):\n",
    "#   - Use the last observed value for a participant with missing data.\n",
    "#   - Consequences: This method might not accurately represent the actual trajectory of the data. If the pattern of missingness is related to the \n",
    "#    variable's change over time, it can introduce bias.\n",
    "\n",
    "# 5. Linear Interpolation:\n",
    "#   - Estimate missing values by linearly interpolating between adjacent observed values.\n",
    "#   - Consequences: This method assumes a linear relationship, which might not be appropriate for all variables. It can also underestimate \n",
    "#    variability and introduce bias if the underlying relationship is not linear.\n",
    "\n",
    "# 6. Multiple Imputation:\n",
    "#   - Generate multiple plausible values for each missing data point, considering the variability in the data.\n",
    "#   - Consequences: This method can provide more accurate estimates and standard errors. However, it requires assumptions about the data's \n",
    "#    distribution and relationships. Multiple imputation is computationally intensive.\n",
    "\n",
    "# 7. Model-Based Imputation:\n",
    "#   - Use regression or other modeling techniques to predict missing values based on other variables.\n",
    "#   - Consequences: This method can yield accurate estimates if the model is well-specified. However, it relies on the validity of the model \n",
    "#    assumptions.\n",
    "\n",
    "# Choosing the appropriate method for handling missing data depends on the nature of the data, the extent of missingness, and the underlying \n",
    "# reasons for missingness. It's important to carefully consider the potential consequences of each method and to document your chosen approach \n",
    "# transparently in your analysis to ensure the robustness of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae08cde-61c8-4ea3-a08a-6aae37fd26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8\n",
    "\n",
    "# Answer 8 -\n",
    "\n",
    "# After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific groups differ \n",
    "# significantly from each other. Common post-hoc tests include:\n",
    "\n",
    "# 1. Tukey's Honestly Significant Difference (HSD):\n",
    "#    - Use when: You have three or more groups and want to compare all possible pairs to identify significant differences.\n",
    "#   - Example: In a study comparing the effectiveness of three different teaching methods on test scores, you find a significant overall effect. \n",
    "#    You would use Tukey's HSD to determine which pairs of teaching methods have significantly different scores.\n",
    "\n",
    "# 2. Bonferroni Correction:\n",
    "#   - Use when: You want to control the familywise error rate (overall Type I error rate) when conducting multiple pairwise comparisons.\n",
    "#   - Example: You're comparing the effectiveness of four different marketing strategies on sales. Using Bonferroni correction can help you \n",
    "#   adjust the significance level for each individual comparison to maintain an overall desired level of significance.\n",
    "\n",
    "# 3. Sidak Correction:\n",
    "#    - Use when: Similar to Bonferroni, but it's less conservative and can be used when there are a larger number of comparisons.\n",
    "#   - Example: You're comparing the performance of different software algorithms across multiple scenarios. The Sidak correction can help you \n",
    "#   adjust p-values for multiple comparisons.\n",
    "\n",
    "# 4. Dunnett's Test:\n",
    "#   - Use when: You have a control group and want to compare other groups to the control while controlling the overall Type I error rate.\n",
    "#   - Example: You're testing the effects of different drug treatments compared to a placebo control. Dunnett's test allows you to focus \n",
    "#    on comparisons with the control while maintaining the overall significance level.\n",
    "\n",
    "# 5. Holm-Bonferroni Method:\n",
    "#   - Use when: Similar to Bonferroni, but it's a step-down procedure that can be more powerful.\n",
    "#   - Example: You're analyzing the effects of different exercise regimes on fitness levels across multiple age groups. The Holm-Bonferroni \n",
    "#    method can provide adjusted p-values for each comparison.\n",
    "\n",
    "# 6. Fisher's Least Significant Difference (LSD):\n",
    "#   - Use when: You have a small number of comparisons and are not concerned about controlling the overall Type I error rate.\n",
    "#   - Example: You're comparing the effectiveness of two treatments on recovery time. Fisher's LSD can help you determine if the treatments \n",
    "#    have significantly different effects.\n",
    "\n",
    "# The choice of post-hoc test depends on your research question, the number of groups, the nature of your data, and whether you need to control\n",
    "# for multiple comparisons. Post-hoc tests help you avoid making overly broad conclusions when you find a significant overall effect in your ANOVA \n",
    "# and allow you to pinpoint which specific group differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0f3bf1-8276-48d5-a053-66f78d74b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 691.1105757931851\n",
      "P-Value: 1.7348315835560967e-75\n",
      "There is a significant difference between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "# Question 9\n",
    "\n",
    "# Answer 9 -\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Weight loss data for each diet\n",
    "diet_a = np.array([2.5, 3.0, 1.8, 2.2, 2.9, 2.7, 2.1, 2.5, 3.2, 3.5,\n",
    "                   2.8, 2.6, 2.3, 2.0, 2.4, 2.6, 3.1, 2.9, 2.7, 3.2,\n",
    "                   2.2, 2.6, 2.8, 2.4, 2.1, 2.9, 2.3, 2.7, 2.5, 2.6,\n",
    "                   2.8, 2.2, 2.6, 2.3, 2.0, 2.7, 3.0, 2.5, 2.8, 2.1,\n",
    "                   2.4, 2.9, 2.6, 2.2, 2.7, 2.3, 2.0, 2.5, 2.8, 2.6])\n",
    "\n",
    "diet_b = np.array([3.8, 3.5, 3.2, 3.7, 3.1, 3.4, 3.9, 3.6, 3.3, 3.1,\n",
    "                   3.7, 3.5, 3.2, 3.6, 3.8, 3.4, 3.9, 3.2, 3.5, 3.7,\n",
    "                   3.1, 3.6, 3.4, 3.2, 3.9, 3.3, 3.5, 3.7, 3.8, 3.4,\n",
    "                   3.6, 3.9, 3.5, 3.2, 3.7, 3.4, 3.1, 3.8, 3.6, 3.2,\n",
    "                   3.5, 3.9, 3.7, 3.4, 3.6, 3.2, 3.8, 3.5, 3.7, 3.9])\n",
    "\n",
    "diet_c = np.array([1.2, 1.5, 1.0, 1.4, 1.3, 1.1, 1.6, 1.7, 1.4, 1.8,\n",
    "                   1.2, 1.3, 1.5, 1.1, 1.6, 1.4, 1.3, 1.7, 1.2, 1.5,\n",
    "                   1.6, 1.0, 1.3, 1.7, 1.4, 1.1, 1.6, 1.2, 1.8, 1.5,\n",
    "                   1.4, 1.3, 1.7, 1.2, 1.1, 1.5, 1.4, 1.8, 1.6, 1.3,\n",
    "                   1.2, 1.7, 1.4, 1.5, 1.1, 1.6, 1.3, 1.8, 1.4, 1.2])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_a, diet_b, diet_c)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4426e4b5-18fa-44f5-99c1-836c1145bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               sum_sq    df         F    PR(>F)\n",
      "C(Software)                 35.723737   2.0  4.579013  0.012956\n",
      "C(Experience)                0.010866   1.0  0.002786  0.958034\n",
      "C(Software):C(Experience)   17.753188   2.0  2.275576  0.109037\n",
      "Residual                   327.668211  84.0       NaN       NaN\n",
      "At least one factor or interaction effect is significant.\n",
      "P-Value of the Factor is:  0.012956222473478022\n",
      "Interaction effect for C(Software) is significant.\n"
     ]
    }
   ],
   "source": [
    "# Question 10\n",
    "\n",
    "# Answer 10 -\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n = 30\n",
    "software = np.repeat(['Program A', 'Program B', 'Program C'], n)\n",
    "experience = np.tile(['Novice', 'Experienced'], 45)\n",
    "time = np.random.normal(loc=10, scale=2, size=n * 3)\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Software': software, 'Experience': experience, 'Time': time})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Report the F-statistics and p-values\n",
    "print(anova_table)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "if any(anova_table['PR(>F)'] < alpha):\n",
    "    print(\"At least one factor or interaction effect is significant.\")\n",
    "    for i in anova_table['PR(>F)']:\n",
    "        if i<alpha:\n",
    "            print(\"P-Value of the Factor is: \",i)\n",
    "    significant_interactions = anova_table[anova_table['PR(>F)'] < alpha]\n",
    "    for index, row in significant_interactions.iterrows():\n",
    "        factors = index.split(':')\n",
    "    print(f\"Interaction effect for {factors[0]} is significant.\")\n",
    "\n",
    "else:\n",
    "    print(\"No significant effects or interactions were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c465dd-504b-4325-aae2-561cd16e5f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "T-Statistic: -1.6677351961320235\n",
      "P-Value: 0.09856078338184605\n",
      "There is no significant difference in test scores between the two groups.\n"
     ]
    }
   ],
   "source": [
    "# Question 11\n",
    "\n",
    "# Answer 11 -\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "control_group = np.random.normal(loc=70, scale=10, size=50)  # Control group test scores\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=50)  # Experimental group test scores\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the t-test results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the two groups.\")\n",
    "    # Perform post-hoc test (Tukey's HSD)\n",
    "    all_scores = np.concatenate([control_group, experimental_group])\n",
    "    group_labels = np.array(['Control'] * len(control_group) + ['Experimental'] * len(experimental_group))\n",
    "    tukey_result = pairwise_tukeyhsd(all_scores, group_labels, alpha=alpha)\n",
    "    print(tukey_result)\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the two groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4226dc0d-414c-4165-832b-3e91d8da601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA results:\n",
      "F-Statistic: 0.8647116816086053\n",
      "P-Value: 0.4247606893565754\n",
      "There is no significant difference in average daily sales between the three stores.\n"
     ]
    }
   ],
   "source": [
    "# Question 12\n",
    "\n",
    "# Answer 12 -\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_days = 30\n",
    "store_a_sales = np.random.normal(loc=1000, scale=200, size=n_days)  # Store A daily sales\n",
    "store_b_sales = np.random.normal(loc=1100, scale=180, size=n_days)  # Store B daily sales\n",
    "store_c_sales = np.random.normal(loc=1050, scale=190, size=n_days)  # Store C daily sales\n",
    "\n",
    "# Combine the sales data\n",
    "all_sales = np.concatenate([store_a_sales, store_b_sales, store_c_sales])\n",
    "store_labels = np.array(['Store A'] * n_days + ['Store B'] * n_days + ['Store C'] * n_days)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(store_a_sales, store_b_sales, store_c_sales)\n",
    "\n",
    "print(\"One-way ANOVA results:\")\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the ANOVA results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in average daily sales between the three stores.\")\n",
    "    # Perform post-hoc test (Tukey's HSD)\n",
    "    tukey_result = pairwise_tukeyhsd(all_sales, store_labels, alpha=alpha)\n",
    "    print(tukey_result)\n",
    "else:\n",
    "    print(\"There is no significant difference in average daily sales between the three stores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c9da4-b0e3-44f0-9a89-b9bd65808692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
